{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AURORA V2 - Meta-Learning Neural Oracle Training\n",
        "\n",
        "This notebook trains the **Neural Oracle** (Layer 2) using curriculum-based meta-learning.\n",
        "\n",
        "## What This Does\n",
        "1. Collects 40+ diverse datasets from OpenML\n",
        "2. Implements curriculum learning (numeric \u2192 categorical \u2192 text)\n",
        "3. Generates ground truth labels by measuring actual ML performance\n",
        "4. Trains XGBoost + LightGBM ensemble\n",
        "5. Exports model for drop-in deployment\n",
        "\n",
        "## Time Estimate\n",
        "- Total: 30-45 minutes\n",
        "- Dataset collection: ~5 min\n",
        "- Meta-learning: ~25 min\n",
        "- Training: ~5 min\n",
        "\n",
        "## Output\n",
        "- `neural_oracle_meta_v3_TIMESTAMP.pkl` - Trained model (~5MB)\n",
        "- Copy to `models/` directory for deployment"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Install Dependencies"
      ],
      "metadata": {
        "id": "cell1_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell1_install"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q xgboost lightgbm scikit-learn pandas numpy openml tqdm\n",
        "\n",
        "# Verify installations\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import sklearn\n",
        "import openml\n",
        "\n",
        "print(f\"\u2705 XGBoost: {xgb.__version__}\")\n",
        "print(f\"\u2705 LightGBM: {lgb.__version__}\")\n",
        "print(f\"\u2705 scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"\u2705 OpenML: {openml.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Clone AURORA Repository"
      ],
      "metadata": {
        "id": "cell2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "REPO_URL = \"https://github.com/shobith-s/AURORA-V2.git\"\n",
        "BRANCH = \"main\"  # Change if using a different branch\n",
        "\n",
        "# Clone repository\n",
        "!git clone {REPO_URL} 2>/dev/null || (cd AURORA-V2 && git pull)\n",
        "%cd AURORA-V2\n",
        "\n",
        "# Checkout branch\n",
        "!git checkout {BRANCH}\n",
        "\n",
        "# Verify\n",
        "!ls -la src/features/"
      ],
      "metadata": {
        "id": "cell2_clone"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Import Modules"
      ],
      "metadata": {
        "id": "cell3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\nimport sys\nimport json\nimport pickle\nimport warnings\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Any, Optional\nfrom datetime import datetime\nfrom dataclasses import dataclass, field\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport openml\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Suppress OpenML warnings about parquet download failures (ARFF fallback works fine)\nimport logging\nlogging.getLogger('openml').setLevel(logging.ERROR)\n\n# Add project to path\nsys.path.insert(0, os.getcwd())\n\n# Import AURORA feature extractor\ntry:\n    from src.features.enhanced_extractor import MetaLearningFeatureExtractor, MetaLearningFeatures\n    print(\"\u2705 MetaLearningFeatureExtractor loaded\")\n    USE_META_FEATURES = True\nexcept ImportError:\n    from src.features.minimal_extractor import MinimalFeatureExtractor as MetaLearningFeatureExtractor\n    print(\"\u26a0\ufe0f Using MinimalFeatureExtractor (fallback)\")\n    USE_META_FEATURES = False\n\nprint(f\"\\n\ud83d\udcc1 Working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "cell3_imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Configuration"
      ],
      "metadata": {
        "id": "cell4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\nclass TrainingConfig:\n    \"\"\"Configuration for meta-learning training.\"\"\"\n    # Dataset collection\n    n_datasets: int = 40              # Number of OpenML datasets\n    max_samples_per_dataset: int = 5000  # Max rows per dataset\n    min_samples_for_cv: int = 50      # Minimum for cross-validation\n    \n    # Cross-validation\n    cv_folds: int = 3                 # Number of CV folds\n    \n    # Training\n    test_size: float = 0.2            # Test set size\n    random_state: int = 42            # Random seed\n    min_confidence: float = 0.5       # Filter low-confidence samples\n    \n    # Actions to try for each column type\n    numeric_actions: List[str] = field(default_factory=lambda: [\n        'keep_as_is',\n        'standard_scale',\n        'minmax_scale',\n        'robust_scale',\n        'log_transform',\n        'log1p_transform',\n        'sqrt_transform',\n        'clip_outliers',\n    ])\n    \n    categorical_actions: List[str] = field(default_factory=lambda: [\n        'keep_as_is',\n        'onehot_encode',\n        'label_encode',\n        'ordinal_encode',\n        'frequency_encode',\n        'drop_column',\n    ])\n    \n    text_actions: List[str] = field(default_factory=lambda: [\n        'keep_as_is',\n        'drop_column',\n        'label_encode',\n    ])\n\n# Initialize configuration\nconfig = TrainingConfig()\n\nprint(\"\ud83d\udccb Configuration:\")\nprint(f\"  Datasets to collect: {config.n_datasets}\")\nprint(f\"  CV folds: {config.cv_folds}\")\nprint(f\"  Random seed: {config.random_state}\")\nprint(f\"  Numeric actions: {len(config.numeric_actions)}\")\nprint(f\"  Categorical actions: {len(config.categorical_actions)}\")"
      ],
      "metadata": {
        "id": "cell4_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5: Dataset Collection\n",
        "\n",
        "Collects 40+ diverse datasets from OpenML for training."
      ],
      "metadata": {
        "id": "cell5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetCollector:\n    \"\"\"Collects datasets from OpenML for meta-learning.\"\"\"\n    \n    # Curated list of diverse OpenML dataset IDs\n    OPENML_DATASETS = [\n        # Classification datasets\n        (31, \"credit-g\"),           # German Credit\n        (37, \"diabetes\"),           # Pima Indians Diabetes\n        (44, \"spambase\"),           # Spam Detection\n        (151, \"electricity\"),       # Electricity\n        (1461, \"bank-marketing\"),   # Bank Marketing\n        (1464, \"blood-transfusion\"), # Blood Transfusion\n        (1480, \"ilpd\"),             # Indian Liver Patient\n        (1494, \"qsar-biodeg\"),      # QSAR Biodegradation\n        (40536, \"SpeedDating\"),     # Speed Dating\n        (40945, \"titanic\"),         # Titanic\n        (41027, \"Australian\"),      # Australian Credit\n        (4134, \"Bioresponse\"),      # Bioresponse\n        (1590, \"adult\"),            # Adult Income\n        (23, \"cmc\"),                # Contraceptive Method\n        (40966, \"MiceProtein\"),     # Mice Protein\n        (1063, \"kc2\"),              # KC2\n        (1068, \"pc1\"),              # PC1\n        (4538, \"GesturePhaseSegmentationProcessed\"),\n        (40981, \"wilt\"),            # Wilt\n        (40982, \"steel-plates-fault\"),\n        (40983, \"wdbc\"),            # Wisconsin Diagnostic Breast Cancer\n        (40984, \"segment\"),         # Image Segmentation\n        (40994, \"climate-model-simulation\"),\n        (1111, \"KDDCup09_appetency\"),\n        (1169, \"airlines\"),         # Airlines\n        (4135, \"Amazon_employee_access\"),\n        (1486, \"nomao\"),            # Nomao\n        (1489, \"phoneme\"),          # Phoneme\n        (1501, \"semeion\"),          # Semeion\n        (23381, \"dresses-sales\"),   # Dresses\n        (42570, \"compass\"),         # COMPAS\n        (6, \"letter\"),              # Letter Recognition\n        (12, \"mfeat-factors\"),      # MFeat Factors\n        (14, \"mfeat-fourier\"),      # MFeat Fourier\n        (16, \"mfeat-karhunen\"),     # MFeat Karhunen\n        (18, \"mfeat-morphological\"),# MFeat Morphological\n        (20, \"mfeat-pixel\"),        # MFeat Pixel\n        (22, \"mfeat-zernike\"),      # MFeat Zernike\n        (32, \"pendigits\"),          # Pen Digits\n        (40499, \"texture\"),         # Texture\n    ]\n    \n    def __init__(self, config: TrainingConfig):\n        self.config = config\n        self.datasets = []\n        \n    def collect(self) -> List[Tuple[str, pd.DataFrame, str]]:\n        \"\"\"Collect datasets from OpenML.\"\"\"\n        collected = []\n        \n        print(f\"\ud83d\udce6 Collecting up to {self.config.n_datasets} datasets from OpenML...\\n\")\n        \n        for dataset_id, name in tqdm(self.OPENML_DATASETS[:self.config.n_datasets], desc=\"Downloading datasets\"):\n            max_retries = 3\n            for attempt in range(max_retries):\n                try:\n                    # Fetch from OpenML\n                    dataset = openml.datasets.get_dataset(dataset_id)\n                    X, y, categorical_indicator, attribute_names = dataset.get_data(\n                        target=dataset.default_target_attribute,\n                        dataset_format='dataframe'\n                    )\n                    \n                    if X is None or y is None:\n                        continue\n                    \n                    # Combine into DataFrame\n                    df = X.copy()\n                    df['target'] = y\n                    \n                    # Reset index to avoid IndexError after sampling\n                    df = df.reset_index(drop=True)\n                    \n                    # Sample if too large\n                    if len(df) > self.config.max_samples_per_dataset:\n                        df = df.sample(n=self.config.max_samples_per_dataset, \n                                       random_state=self.config.random_state).reset_index(drop=True)\n                    \n                    collected.append((name, df, 'classification'))\n                    print(f\"  \u2713 Downloaded {name}: {len(df)} rows, {len(df.columns)} columns\")\n                    \n                except Exception as e:\n                    if attempt < max_retries - 1:\n                        continue\n                    print(f\"  \u26a0\ufe0f Skipped {name}: {str(e)[:50]}\")\n                    break\n                else:\n                    # Success - break out of retry loop\n                    break\n        \n        print(f\"\\n\u2705 Collected {len(collected)} datasets\")\n        self.datasets = collected\n        return collected\n\n# Collect datasets\ncollector = DatasetCollector(config)\ndatasets = collector.collect()\n\n# Summary\ntotal_rows = sum(len(df) for _, df, _ in datasets)\ntotal_cols = sum(len(df.columns) for _, df, _ in datasets)\nprint(f\"\\n\ud83d\udcca Dataset Summary:\")\nprint(f\"  Total datasets: {len(datasets)}\")\nprint(f\"  Total rows: {total_rows:,}\")\nprint(f\"  Total columns: {total_cols:,}\")"
      ],
      "metadata": {
        "id": "cell5_collect"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 6: Curriculum Meta-Learner\n",
        "\n",
        "Core meta-learning logic with staged curriculum."
      ],
      "metadata": {
        "id": "cell6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\nclass TrainingSample:\n    \"\"\"A single training sample.\"\"\"\n    features: np.ndarray\n    label: str\n    confidence: float\n    column_type: str\n    column_name: str\n    dataset_name: str\n    performance_score: float\n\n\nclass PreprocessingExecutor:\n    \"\"\"Executes preprocessing actions on columns.\"\"\"\n    \n    @staticmethod\n    def apply(column: pd.Series, action: str) -> Optional[np.ndarray]:\n        \"\"\"Apply a preprocessing action to a column.\"\"\"\n        non_null = column.dropna()\n        if len(non_null) == 0:\n            return None\n        \n        try:\n            if action == 'keep_as_is':\n                if pd.api.types.is_numeric_dtype(column):\n                    return column.fillna(0).values.reshape(-1, 1)\n                else:\n                    le = LabelEncoder()\n                    return le.fit_transform(column.fillna('__NA__').astype(str)).reshape(-1, 1)\n            \n            elif action == 'standard_scale':\n                if not pd.api.types.is_numeric_dtype(column):\n                    return None\n                scaler = StandardScaler()\n                values = column.fillna(column.mean()).values.reshape(-1, 1)\n                return scaler.fit_transform(values)\n            \n            elif action == 'minmax_scale':\n                if not pd.api.types.is_numeric_dtype(column):\n                    return None\n                scaler = MinMaxScaler()\n                values = column.fillna(column.mean()).values.reshape(-1, 1)\n                return scaler.fit_transform(values)\n            \n            elif action == 'robust_scale':\n                if not pd.api.types.is_numeric_dtype(column):\n                    return None\n                from sklearn.preprocessing import RobustScaler\n                scaler = RobustScaler()\n                values = column.fillna(column.median()).values.reshape(-1, 1)\n                return scaler.fit_transform(values)\n            \n            elif action == 'log_transform':\n                if not pd.api.types.is_numeric_dtype(column):\n                    return None\n                values = column.fillna(0)\n                if (values <= 0).any():\n                    return None\n                return np.log(values).values.reshape(-1, 1)\n            \n            elif action == 'log1p_transform':\n                if not pd.api.types.is_numeric_dtype(column):\n                    return None\n                values = column.fillna(0)\n                if (values < 0).any():\n                    return None\n                return np.log1p(values).values.reshape(-1, 1)\n            \n            \n            elif action == 'sqrt_transform':\n                if not pd.api.types.is_numeric_dtype(column):\n                    return None\n                values = column.fillna(0)\n                if (values < 0).any():\n                    return None\n                return np.sqrt(values).values.reshape(-1, 1)\n            elif action == 'clip_outliers':\n                if not pd.api.types.is_numeric_dtype(column):\n                    return None\n                values = column.fillna(column.median())\n                q1, q3 = values.quantile([0.25, 0.75])\n                iqr = q3 - q1\n                clipped = values.clip(q1 - 1.5*iqr, q3 + 1.5*iqr)\n                return clipped.values.reshape(-1, 1)\n            \n            elif action == 'onehot_encode':\n                values = column.fillna('__NA__').astype(str)\n                dummies = pd.get_dummies(values)\n                return dummies.values\n            \n            elif action == 'label_encode':\n                values = column.fillna('__NA__').astype(str)\n                le = LabelEncoder()\n                return le.fit_transform(values).reshape(-1, 1)\n            \n            elif action == 'ordinal_encode':\n                from sklearn.preprocessing import OrdinalEncoder\n                values = column.fillna('__NA__').astype(str).values.reshape(-1, 1)\n                enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n                return enc.fit_transform(values)\n            \n            elif action == 'frequency_encode':\n                values = column.fillna('__NA__').astype(str)\n                freq = values.value_counts(normalize=True)\n                return values.map(freq).values.reshape(-1, 1)\n            \n            elif action == 'drop_column':\n                return None\n            \n            else:\n                return None\n                \n        except Exception:\n            return None\n\n\nclass CurriculumMetaLearner:\n    \"\"\"Curriculum-based meta-learner for neural oracle training.\"\"\"\n    \n    def __init__(self, config: TrainingConfig):\n        self.config = config\n        self.extractor = MetaLearningFeatureExtractor()\n        self.executor = PreprocessingExecutor()\n        self.samples: List[TrainingSample] = []\n        \n    def classify_column(self, column: pd.Series, name: str) -> str:\n        \"\"\"Classify column as numeric, categorical, text, or deterministic.\"\"\"\n        if column.isna().all():\n            return 'deterministic:all_null'\n        if column.nunique() <= 1:\n            return 'deterministic:constant'\n        if column.nunique() == len(column.dropna()) and 'id' in name.lower():\n            return 'deterministic:id'\n        if pd.api.types.is_datetime64_any_dtype(column):\n            return 'deterministic:datetime'\n        if pd.api.types.is_bool_dtype(column):\n            return 'deterministic:boolean'\n        \n        # Check for boolean-like strings\n        if pd.api.types.is_object_dtype(column):\n            unique_lower = set(column.dropna().astype(str).str.lower().unique())\n            if unique_lower.issubset({'true', 'false', 'yes', 'no', '0', '1', 't', 'f', 'y', 'n'}):\n                if len(unique_lower) <= 3:\n                    return 'deterministic:boolean'\n        \n        if pd.api.types.is_numeric_dtype(column):\n            return 'numeric'\n        \n        # Check for text vs categorical\n        non_null = column.dropna()\n        if len(non_null) > 0:\n            avg_len = non_null.astype(str).str.len().mean()\n            unique_ratio = column.nunique() / len(non_null)\n            if unique_ratio > 0.5 and avg_len > 30:\n                return 'text'\n        \n        return 'categorical'\n    \n    def measure_performance(self, X: np.ndarray, y: np.ndarray) -> Optional[float]:\n        \"\"\"Measure cross-validation performance.\"\"\"\n        if len(y) < self.config.min_samples_for_cv:\n            return None\n        \n        # Encode target if needed\n        if not pd.api.types.is_numeric_dtype(pd.Series(y)):\n            le = LabelEncoder()\n            y = le.fit_transform(y.astype(str))\n        \n        # Check for minimum class samples\n        unique, counts = np.unique(y, return_counts=True)\n        if len(unique) < 2 or min(counts) < self.config.cv_folds:\n            return None\n        \n        try:\n            cv = StratifiedKFold(\n                n_splits=self.config.cv_folds,\n                shuffle=True,\n                random_state=self.config.random_state\n            )\n            \n            model = LogisticRegression(\n                max_iter=500,\n                random_state=self.config.random_state,\n                n_jobs=-1\n            )\n            \n            scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n            return float(scores.mean())\n        except Exception:\n            return None\n    \n    def find_best_action(\n        self,\n        df: pd.DataFrame,\n        col_name: str,\n        target: pd.Series,\n        actions: List[str],\n        other_cols: Optional[pd.DataFrame] = None\n    ) -> Tuple[str, float, Dict[str, float]]:\n        \"\"\"Find best action by measuring performance.\"\"\"\n        scores = {}\n        column = df[col_name]\n        \n        # Prepare target\n        mask = ~target.isna()\n        y = target[mask].values\n        \n        for action in actions:\n            transformed = self.executor.apply(column, action)\n            \n            if action == 'drop_column' or transformed is None:\n                if other_cols is not None and len(other_cols.columns) > 0:\n                    X = other_cols[mask].values\n                else:\n                    continue\n            else:\n                X_col = transformed[mask]\n                if other_cols is not None and len(other_cols.columns) > 0:\n                    X = np.hstack([other_cols[mask].values, X_col])\n                else:\n                    X = X_col\n            \n            score = self.measure_performance(X, y)\n            if score is not None:\n                scores[action] = score\n        \n        if not scores:\n            return 'keep_as_is', 0.0, {}\n        \n        best = max(scores, key=scores.get)\n        return best, scores[best], scores\n    \n    def extract_features(self, column: pd.Series, name: str) -> np.ndarray:\n        \"\"\"Extract features from a column.\"\"\"\n        if USE_META_FEATURES and hasattr(self.extractor, 'extract_meta_features'):\n            features = self.extractor.extract_meta_features(column, name)\n        else:\n            features = self.extractor.extract(column, name)\n        return features.to_array()\n    \n    def process_dataset(self, name: str, df: pd.DataFrame) -> List[TrainingSample]:\n        \"\"\"Process a single dataset through curriculum stages.\"\"\"\n        samples = []\n        \n        if 'target' not in df.columns:\n            return samples\n        \n        target = df['target']\n        feature_cols = [c for c in df.columns if c != 'target']\n        \n        # Classify columns\n        col_types = {c: self.classify_column(df[c], c) for c in feature_cols}\n        \n        # Stage 1: Deterministic\n        det_rules = {\n            'all_null': 'drop_column',\n            'constant': 'drop_column',\n            'id': 'drop_column',\n            'datetime': 'parse_datetime',\n            'boolean': 'parse_boolean',\n        }\n        \n        for col, ctype in col_types.items():\n            if ctype.startswith('deterministic:'):\n                rule = ctype.split(':')[1]\n                action = det_rules.get(rule, 'keep_as_is')\n                features = self.extract_features(df[col], col)\n                samples.append(TrainingSample(\n                    features=features,\n                    label=action,\n                    confidence=1.0,\n                    column_type='deterministic',\n                    column_name=col,\n                    dataset_name=name,\n                    performance_score=1.0\n                ))\n        \n        # Prepare processed numeric columns for context\n        numeric_cols = [c for c, t in col_types.items() if t == 'numeric']\n        processed_numeric = pd.DataFrame()\n        \n        # Stage 2: Numeric\n        for col in numeric_cols:\n            best, score, all_scores = self.find_best_action(\n                df, col, target,\n                self.config.numeric_actions,\n                processed_numeric if len(processed_numeric.columns) > 0 else None\n            )\n            \n            if all_scores:\n                # Calculate confidence from score gap\n                sorted_scores = sorted(all_scores.values(), reverse=True)\n                gap = sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else 0.3\n                confidence = min(1.0, 0.5 + gap * 5)\n                \n                features = self.extract_features(df[col], col)\n                samples.append(TrainingSample(\n                    features=features,\n                    label=best,\n                    confidence=confidence,\n                    column_type='numeric',\n                    column_name=col,\n                    dataset_name=name,\n                    performance_score=score\n                ))\n                \n                # Add to context\n                scaled = self.executor.apply(df[col], 'standard_scale')\n                if scaled is not None:\n                    processed_numeric[col] = scaled.flatten()\n        \n        # Stage 3: Categorical\n        cat_cols = [c for c, t in col_types.items() if t == 'categorical']\n        for col in cat_cols:\n            best, score, all_scores = self.find_best_action(\n                df, col, target,\n                self.config.categorical_actions,\n                processed_numeric if len(processed_numeric.columns) > 0 else None\n            )\n            \n            if all_scores:\n                sorted_scores = sorted(all_scores.values(), reverse=True)\n                gap = sorted_scores[0] - sorted_scores[1] if len(sorted_scores) > 1 else 0.3\n                confidence = min(1.0, 0.5 + gap * 5)\n                \n                features = self.extract_features(df[col], col)\n                samples.append(TrainingSample(\n                    features=features,\n                    label=best,\n                    confidence=confidence,\n                    column_type='categorical',\n                    column_name=col,\n                    dataset_name=name,\n                    performance_score=score\n                ))\n        \n        # Stage 4: Text\n        text_cols = [c for c, t in col_types.items() if t == 'text']\n        for col in text_cols:\n            best, score, all_scores = self.find_best_action(\n                df, col, target,\n                self.config.text_actions,\n                processed_numeric if len(processed_numeric.columns) > 0 else None\n            )\n            \n            if not all_scores:\n                best = 'drop_column'\n                score = 0.0\n                all_scores = {'drop_column': 0.0}\n            \n            confidence = 0.7  # Text is often dropped\n            features = self.extract_features(df[col], col)\n            samples.append(TrainingSample(\n                features=features,\n                label=best,\n                confidence=confidence,\n                column_type='text',\n                column_name=col,\n                dataset_name=name,\n                performance_score=score\n            ))\n        \n        return samples\n    \n    def run(self, datasets: List[Tuple[str, pd.DataFrame, str]]) -> List[TrainingSample]:\n        \"\"\"Run curriculum meta-learning on all datasets.\"\"\"\n        print(f\"\\n\ud83c\udf93 Starting Curriculum Meta-Learning...\\n\")\n        \n        for name, df, _ in tqdm(datasets, desc=\"Processing datasets\"):\n            samples = self.process_dataset(name, df)\n            self.samples.extend(samples)\n        \n        # Filter by minimum confidence\n        self.samples = [\n            s for s in self.samples \n            if s.confidence >= self.config.min_confidence\n        ]\n        \n        print(f\"\\n\u2705 Generated {len(self.samples)} training samples\")\n        return self.samples\n\nprint(\"\u2705 CurriculumMetaLearner defined\")"
      ],
      "metadata": {
        "id": "cell6_learner"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 7-9: Run Curriculum Stages\n\n### What This Does\nThis cell runs the **curriculum-based meta-learning** process:\n\n1. **Collects 40+ datasets** from OpenML with diverse characteristics\n2. **For each column** in each dataset:\n   - Classifies column type (numeric, categorical, text, or deterministic)\n   - **Measures actual ML performance** by trying different preprocessing actions\n   - Uses cross-validation with LogisticRegression to evaluate each action\n   - Selects the best action based on CV accuracy\n   - Calculates confidence from the performance gap\n3. **Generates training samples** with:\n   - 62 comprehensive meta-features\n   - Performance-based labels (best action)\n   - Confidence scores\n4. **Filters samples** by minimum confidence threshold (0.5)\n\n### Why This Takes Time (30-60 minutes)\nWe're running **cross-validation** on every column to find which preprocessing action actually improves ML performance. This is much better than heuristics, but requires computational time.\n\n**Progress bar** will show dataset processing status below.\n\n### Expected Output\n- 1000-3000 training samples\n- Diverse action distribution (15+ different actions)\n- High-confidence labels based on actual performance\n"
      ],
      "metadata": {
        "id": "cell7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\u26a0\ufe0f  WARNING: This training will take 30-60 minutes\")\nprint(\"   We're running cross-validation on 40+ datasets to measure ACTUAL performance.\")\nprint(\"   This is necessary for learning which preprocessing actions work best.\")\nprint(\"   Progress will be shown below...\\n\")\n\n# Initialize and run meta-learner\nlearner = CurriculumMetaLearner(config)\nsamples = learner.run(datasets)\n\n# Statistics\nlabels = [s.label for s in samples]\ntypes = [s.column_type for s in samples]\nconfidences = [s.confidence for s in samples]\n\nprint(\"\\n\ud83d\udcca Training Data Statistics:\")\nprint(f\"  Total samples: {len(samples)}\")\nprint(f\"  Average confidence: {np.mean(confidences):.3f}\")\nprint(f\"  Unique labels: {len(set(labels))}\")\nprint(f\"  Unique datasets: {len(set(s.dataset_name for s in samples))}\")\n\nprint(\"\\n\ud83d\udccb Label Distribution:\")\nfor label, count in sorted(Counter(labels).items(), key=lambda x: -x[1]):\n    pct = count / len(labels) * 100\n    print(f\"  {label:25s}: {count:4d} ({pct:5.1f}%)\")\n\nprint(\"\\n\ud83d\udccb Type Distribution:\")\nfor ctype, count in sorted(Counter(types).items(), key=lambda x: -x[1]):\n    pct = count / len(types) * 100\n    print(f\"  {ctype:15s}: {count:4d} ({pct:5.1f}%)\")"
      ],
      "metadata": {
        "id": "cell7_run"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 10: Prepare Training Data"
      ],
      "metadata": {
        "id": "cell10_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare feature matrix and labels\n",
        "X = np.vstack([s.features for s in samples])\n",
        "y = np.array([s.label for s in samples])\n",
        "sample_weights = np.array([s.confidence for s in samples])\n",
        "\n",
        "# Get feature names\n",
        "if USE_META_FEATURES:\n",
        "    feature_names = MetaLearningFeatures.get_feature_names()\n",
        "else:\n",
        "    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "print(f\"\ud83d\udcca Training Data Shape:\")\n",
        "print(f\"  Features (X): {X.shape}\")\n",
        "print(f\"  Labels (y): {y.shape}\")\n",
        "print(f\"  Feature count: {len(feature_names)}\")\n",
        "print(f\"  Unique labels: {len(np.unique(y))}\")\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
        "    X, y, sample_weights,\n",
        "    test_size=config.test_size,\n",
        "    random_state=config.random_state,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Train/Test Split:\")\n",
        "print(f\"  Training: {len(X_train)} samples\")\n",
        "print(f\"  Test: {len(X_test)} samples\")"
      ],
      "metadata": {
        "id": "cell10_prepare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 11: Train XGBoost + LightGBM Ensemble"
      ],
      "metadata": {
        "id": "cell11_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\ud83d\ude80 Training XGBoost + LightGBM Ensemble...\\n\")\n",
        "\n",
        "# XGBoost classifier\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=config.random_state,\n",
        "    n_jobs=-1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "# LightGBM classifier\n",
        "lgb_model = LGBMClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=config.random_state,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# Voting ensemble\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgb', lgb_model)\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[0.6, 0.4]\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Training ensemble (this may take a few minutes)...\")\n",
        "ensemble.fit(X_train, y_train, sample_weight=w_train)\n",
        "\n",
        "# Evaluate\n",
        "train_pred = ensemble.predict(X_train)\n",
        "test_pred = ensemble.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, train_pred)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(f\"\\n\u2705 Training Complete!\")\n",
        "print(f\"  Training Accuracy: {train_acc:.1%}\")\n",
        "print(f\"  Test Accuracy: {test_acc:.1%}\")"
      ],
      "metadata": {
        "id": "cell11_train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 12: Evaluate on Test Set"
      ],
      "metadata": {
        "id": "cell12_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\ud83d\udcca Detailed Evaluation:\\n\")\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, test_pred))\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"\\nPer-Class Accuracy:\")\n",
        "classes = np.unique(y_test)\n",
        "for cls in sorted(classes):\n",
        "    mask = y_test == cls\n",
        "    cls_acc = accuracy_score(y_test[mask], test_pred[mask])\n",
        "    count = mask.sum()\n",
        "    print(f\"  {cls:25s}: {cls_acc:5.1%} ({count:3d} samples)\")\n",
        "\n",
        "# Confusion matrix summary\n",
        "print(\"\\n\ud83d\udcca Model Quality Check:\")\n",
        "print(f\"  \u2705 Test accuracy: {test_acc:.1%}\")\n",
        "print(f\"  {'\u2705' if test_acc >= 0.9 else '\u26a0\ufe0f'} Target: 90%+\")\n",
        "print(f\"  \u2705 Unique classes: {len(classes)}\")"
      ],
      "metadata": {
        "id": "cell12_eval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 13: Save Model + Metadata"
      ],
      "metadata": {
        "id": "cell13_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate filename with timestamp\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "model_filename = f'neural_oracle_meta_v3_{timestamp}.pkl'\n",
        "\n",
        "# Prepare metadata\n",
        "metadata = {\n",
        "    'version': 'meta_v3',\n",
        "    'accuracy': float(test_acc),\n",
        "    'training_samples': len(samples),\n",
        "    'datasets': len(datasets),\n",
        "    'feature_count': X.shape[1],\n",
        "    'unique_labels': len(np.unique(y)),\n",
        "    'timestamp': timestamp,\n",
        "    'config': {\n",
        "        'n_datasets': config.n_datasets,\n",
        "        'cv_folds': config.cv_folds,\n",
        "        'random_state': config.random_state,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save model directly (NeuralOracle compatible format)\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(ensemble, f)\n",
        "\n",
        "# Save metadata separately\n",
        "metadata_filename = f'model_metadata_{timestamp}.json'\n",
        "with open(metadata_filename, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# Get file size\n",
        "import os\n",
        "model_size = os.path.getsize(model_filename) / (1024 * 1024)\n",
        "\n",
        "print(f\"\u2705 Model saved: {model_filename}\")\n",
        "print(f\"\u2705 Metadata saved: {metadata_filename}\")\n",
        "print(f\"\\n\ud83d\udcca Model Info:\")\n",
        "print(f\"  Size: {model_size:.1f} MB\")\n",
        "print(f\"  Test Accuracy: {test_acc:.1%}\")\n",
        "print(f\"  Training Samples: {len(samples)}\")\n",
        "print(f\"  Datasets: {len(datasets)}\")\n",
        "print(f\"  Features: {X.shape[1]}\")"
      ],
      "metadata": {
        "id": "cell13_save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 14: Download Instructions"
      ],
      "metadata": {
        "id": "cell14_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files (Colab only)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"\ud83d\udce5 Downloading model...\")\n",
        "    files.download(model_filename)\n",
        "    \n",
        "    print(\"\\n\ud83d\udce5 Downloading metadata...\")\n",
        "    files.download(metadata_filename)\n",
        "    \n",
        "    print(\"\\n\u2705 Downloads started!\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"\ud83d\udcc1 Not running in Colab. Files saved locally:\")\n",
        "    print(f\"  - {model_filename}\")\n",
        "    print(f\"  - {metadata_filename}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DEPLOYMENT INSTRUCTIONS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\"\"\n",
        "1. Copy the model file to your AURORA installation:\n",
        "   \n",
        "   cp {model_filename} /path/to/AURORA-V2/models/\n",
        "\n",
        "2. The NeuralOracle will automatically load the latest model,\n",
        "   or you can specify the path explicitly:\n",
        "   \n",
        "   from src.neural.oracle import NeuralOracle\n",
        "   oracle = NeuralOracle(model_path='models/{model_filename}')\n",
        "\n",
        "3. Verify deployment:\n",
        "   \n",
        "   python scripts/model_integration_utils.py\n",
        "\n",
        "Expected Results:\n",
        "  - Model loads without errors\n",
        "  - Bestsellers.csv accuracy: 90%+\n",
        "  - Inference time: <5ms per column\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "cell14_download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n\n### What Was Created\n- **Trained Model**: XGBoost + LightGBM ensemble for preprocessing action prediction\n- **Training Data**: Generated from 40+ OpenML datasets using **performance-based ground truth** (actual CV scores)\n- **Features**: 62 comprehensive meta-features covering statistics, semantics, patterns, and distributions\n- **Actions**: 15+ preprocessing actions learned through curriculum meta-learning\n\n### Key Improvements in This Version\n\u2713 **All 15+ preprocessing actions** are now being learned (not just 3!)\n\u2713 **Performance-based action selection** using cross-validation (not heuristics)\n\u2713 **30+ comprehensive meta-features** extracted per column\n\u2713 **IndexError bug fixed** with `reset_index(drop=True)`\n\u2713 **Retry logic** for failed dataset downloads\n\u2713 **OpenML warning suppression** for cleaner output\n\u2713 **Training time: 30-60 minutes** (necessary for actual performance measurement)\n\n### Expected Action Distribution\nAfter training, you should see a diverse distribution like:\n```\n\ud83d\udcca Action Distribution:\n   - standard_scale: ~30-40%\n   - keep_as_is: ~10-15%\n   - log1p_transform: ~8-12%\n   - robust_scale: ~8-10%\n   - onehot_encode: ~8-10%\n   - label_encode: ~6-8%\n   - clip_outliers: ~5-8%\n   - frequency_encode: ~3-5%\n   - minmax_scale: ~3-5%\n   - drop_column: ~2-4%\n   - ordinal_encode: ~2-4%\n   - sqrt_transform: ~2-3%\n   - log_transform: ~1-3%\n```\n\nIf you see only 3 actions (standard_scale, onehot_encode, label_encode), the training didn't work correctly.\n\n### Model Details\n- **Architecture**: VotingClassifier (XGBoost 60%, LightGBM 40%)\n- **All Actions**: keep_as_is, standard_scale, minmax_scale, robust_scale, log_transform, log1p_transform, sqrt_transform, clip_outliers, onehot_encode, label_encode, ordinal_encode, frequency_encode, drop_column\n- **Expected accuracy**: 85-92% on test data\n- **Training samples**: 1000-3000 column examples\n\n### Next Steps\n1. Download the model file (`neural_oracle_meta_v3_TIMESTAMP.pkl`)\n2. Copy to `models/` directory in your AURORA installation\n3. Run validation tests: `python tests/test_complete_system.py`\n4. Deploy to production\n\n### Troubleshooting\n- **If accuracy is low (<80%)**: Try increasing `n_datasets` in config or reducing `min_confidence`\n- **If training is slow**: This is expected! We're running CV on 40+ datasets. Reduce `n_datasets` if needed.\n- **If only 3 actions learned**: Check that `find_best_action()` is being called, not heuristics\n- **If IndexError occurs**: Make sure `reset_index(drop=True)` is called after sampling\n- Check `docs/META_LEARNING_GUIDE.md` for detailed documentation\n"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}