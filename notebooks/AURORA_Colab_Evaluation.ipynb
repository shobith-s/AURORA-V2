{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AURORA V2 - Comprehensive Evaluation\n",
        "\n",
        "This notebook runs the complete AURORA V2 evaluation pipeline:\n",
        "\n",
        "1. **Quick Validation** (5 min): Validate against ground truth labels\n",
        "2. **Full Evaluation** (2-3 hours): Ablation study, benchmarks, statistical analysis\n",
        "\n",
        "All results are publication-ready with tables, figures, and PDF report."
      ],
      "metadata": {
        "id": "aurora_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Setup\n",
        "\n",
        "Clone repository and install dependencies."
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_cell"
      },
      "outputs": [],
      "source": [
        "# Configuration - modify these variables as needed\n",
        "REPO_URL = \"https://github.com/shobith-s/AURORA-V2.git\"  # Repository URL\n",
        "BRANCH = \"main\"  # Branch to checkout (e.g., 'main', 'polishing', 'develop')\n",
        "\n",
        "# Clone AURORA V2 repository\n",
        "!git clone {REPO_URL}\n",
        "%cd AURORA-V2\n",
        "\n",
        "# Checkout the specified branch\n",
        "!git checkout {BRANCH}\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "# Verify installation\n",
        "!python -c \"from src.core.preprocessor import IntelligentPreprocessor; print('‚úÖ AURORA loaded successfully')\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Quick Validation (~5 minutes)\n",
        "\n",
        "Validates AURORA predictions against ground truth labels.\n",
        "Generates accuracy metrics and confusion matrix."
      ],
      "metadata": {
        "id": "validation_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run ground truth validation\n",
        "# Use --create-sample to generate sample data if validated_labels.json doesn't exist\n",
        "!python scripts/validate_against_ground_truth.py --create-sample\n",
        "\n",
        "# Display results\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "results_path = Path('results/ground_truth_validation.json')\n",
        "if results_path.exists():\n",
        "    with open(results_path) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GROUND TRUTH VALIDATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nAccuracy: {results['test_accuracy']:.2%}\")\n",
        "    print(f\"Correct: {results['correct_predictions']}/{results['total_examples']}\")\n",
        "    print(f\"Skipped: {results['skipped_examples']}\")\n",
        "    \n",
        "    print(\"\\nPer-Action Metrics:\")\n",
        "    for action, metrics in sorted(results['per_action_metrics'].items(), \n",
        "                                   key=lambda x: x[1]['support'], reverse=True)[:10]:\n",
        "        print(f\"  {action}: F1={metrics['f1']:.3f}, Support={metrics['support']}\")"
      ],
      "metadata": {
        "id": "validation_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Full Evaluation (~2-3 hours)\n",
        "\n",
        "Runs complete evaluation pipeline:\n",
        "- Downloads OpenML datasets\n",
        "- Ablation study (4 variants)\n",
        "- Comprehensive benchmarks\n",
        "- Statistical analysis\n",
        "- Generates tables, figures, and PDF report"
      ],
      "metadata": {
        "id": "full_eval_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run full evaluation pipeline\n",
        "# --datasets: Number of datasets (5 for quick, 10 for thorough)\n",
        "# --verbose: Enable detailed output\n",
        "!python scripts/run_colab_evaluation.py --datasets 5 --verbose"
      ],
      "metadata": {
        "id": "full_eval_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Display Results\n",
        "\n",
        "Show tables and figures inline."
      ],
      "metadata": {
        "id": "display_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown, Image\n",
        "from pathlib import Path\n",
        "\n",
        "# Display paper tables\n",
        "tables_path = Path('results/paper_tables.md')\n",
        "if tables_path.exists():\n",
        "    print(\"üìä Paper Tables\")\n",
        "    print(\"=\"*60)\n",
        "    display(Markdown(tables_path.read_text()))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Tables not found. Run Cell 3 first.\")"
      ],
      "metadata": {
        "id": "display_tables_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display figures\n",
        "from IPython.display import display, Image\n",
        "from pathlib import Path\n",
        "\n",
        "figures_dir = Path('results/figures')\n",
        "\n",
        "figures = [\n",
        "    ('Accuracy Comparison', 'accuracy_comparison.png'),\n",
        "    ('Decision Sources', 'decision_sources.png'),\n",
        "    ('Latency Distribution', 'latency_distribution.png'),\n",
        "    ('Confusion Matrix', 'confusion_matrix.png'),\n",
        "]\n",
        "\n",
        "for title, filename in figures:\n",
        "    fig_path = figures_dir / filename\n",
        "    if fig_path.exists():\n",
        "        print(f\"\\nüìà {title}\")\n",
        "        print(\"=\"*60)\n",
        "        display(Image(fig_path))\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {title} not found\")"
      ],
      "metadata": {
        "id": "display_figures_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5: Download Results\n",
        "\n",
        "Download all results as a ZIP file."
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ZIP archive and download\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "results_dir = Path('results')\n",
        "\n",
        "if results_dir.exists():\n",
        "    # Create ZIP\n",
        "    shutil.make_archive('evaluation_results', 'zip', results_dir)\n",
        "    print(\"‚úÖ Created evaluation_results.zip\")\n",
        "    \n",
        "    # List contents\n",
        "    print(\"\\nContents:\")\n",
        "    for f in sorted(results_dir.rglob('*')):\n",
        "        if f.is_file():\n",
        "            size_kb = f.stat().st_size / 1024\n",
        "            print(f\"  ‚Ä¢ {f.relative_to(results_dir)} ({size_kb:.1f} KB)\")\n",
        "    \n",
        "    # Download (Colab only)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download('evaluation_results.zip')\n",
        "        print(\"\\n‚úÖ Download started!\")\n",
        "    except ImportError:\n",
        "        print(\"\\nüìÅ Not in Colab. Find results in: ./results/\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Results directory not found. Run evaluation first.\")"
      ],
      "metadata": {
        "id": "download_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "After running this notebook, you will have:\n",
        "\n",
        "1. **Ground Truth Validation** (results/ground_truth_validation.json)\n",
        "   - Accuracy against validated labels\n",
        "   - Per-action precision/recall/F1\n",
        "   - Confusion matrix\n",
        "\n",
        "2. **Ablation Study** (results/ablation_results.json)\n",
        "   - Random baseline vs Symbolic-only vs Neural-only vs AURORA Hybrid\n",
        "   - Per-dataset accuracy comparison\n",
        "\n",
        "3. **Benchmarks** (results/benchmark_results.json)\n",
        "   - Decision source breakdown (symbolic vs neural)\n",
        "   - Latency metrics\n",
        "   - Confidence statistics\n",
        "\n",
        "4. **Statistical Tests** (results/statistical_tests.json)\n",
        "   - T-tests between variants\n",
        "   - P-values and effect sizes\n",
        "\n",
        "5. **Publication-Ready Outputs**\n",
        "   - Tables in Markdown (paper_tables.md)\n",
        "   - Figures in PNG (figures/)\n",
        "   - PDF report (EVALUATION_REPORT.pdf)"
      ],
      "metadata": {
        "id": "summary_header"
      }
    }
  ]
}
